"""
FastAPI Agent Server
æä¾› HTTP API æ¥å£ä¾› Java åç«¯è°ƒç”¨
æ”¯æŒ --stdio æ¨¡å¼å¯ç”¨ LangGraph STDIO
æ”¯æŒæµå¼è¾“å‡º (SSE)
"""

import os
# ç¦ç”¨ tokenizers å¹¶è¡ŒåŒ–è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import sys
import json
import asyncio
import argparse
import threading
from typing import Optional, List, AsyncGenerator
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import uvicorn

from chatbot import ChatbotWithMemory
from langgraph_agent import LangGraphAgent
from tools import FileHandler


# é…ç½®: é€‰æ‹©ä½¿ç”¨å“ªä¸ª agent (é»˜è®¤ä½¿ç”¨LangGraph)
USE_LANGGRAPH = os.getenv("USE_LANGGRAPH", "true").lower() == "true"

# å…¨å±€ agent å®ä¾‹
chatbot: Optional[ChatbotWithMemory] = None
langgraph_agent: Optional[LangGraphAgent] = None
file_handler: Optional[FileHandler] = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global chatbot, langgraph_agent, file_handler
    
    # åˆå§‹åŒ–æ–‡ä»¶å¤„ç†å™¨
    file_handler = FileHandler(workspace_dir="./workspace")
    print("âœ… æ–‡ä»¶å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    if USE_LANGGRAPH:
        print("æ­£åœ¨åˆå§‹åŒ– LangGraph Agent...")
        try:
            langgraph_agent = LangGraphAgent(
                memory_dir="./chat_memory_db",
                workspace_dir="./workspace"
            )
            print("âœ… LangGraph Agent åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    else:
        print("æ­£åœ¨åˆå§‹åŒ– Chatbot Agent...")
        try:
            chatbot = ChatbotWithMemory(
                memory_dir="./chat_memory_db",
                short_term_limit=10,
                retrieve_memories=5
            )
            print("âœ… Chatbot Agent åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    yield
    
    print("æ­£åœ¨å…³é—­ Agent...")


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Chatbot Agent API",
    description="å¸¦æœ‰é•¿æ—¶è®°å¿†çš„å¯¹è¯æœºå™¨äºº Agent æœåŠ¡ (æ”¯æŒ LangGraph)",
    version="2.0.0",
    lifespan=lifespan
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# è¯·æ±‚/å“åº”æ¨¡å‹
class ChatRequest(BaseModel):
    message: str = Field(..., description="ç”¨æˆ·æ¶ˆæ¯")
    session_id: str = Field(default="default", description="ä¼šè¯ID")
    enable_web_search: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢")
    deep_think: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒ(TOT)")
    thought_branches: int = Field(default=5, description="æ€è€ƒåˆ†æ”¯æ•°é‡")
    thought_depth: int = Field(default=3, description="æ€è€ƒæ·±åº¦")


class ChatResponse(BaseModel):
    response: str = Field(..., description="åŠ©æ‰‹å›å¤")
    session_id: str = Field(..., description="ä¼šè¯ID")
    thinking_process: str = Field(default="", description="TOTæ€è€ƒè¿‡ç¨‹")
    tot_score: float = Field(default=0.0, description="TOTæœ€ä½³å¾—åˆ†")
    deep_think: bool = Field(default=False, description="æ˜¯å¦ä½¿ç”¨äº†æ·±åº¦æ€è€ƒ")


class MemoryStatsResponse(BaseModel):
    long_term_memories: int = Field(..., description="é•¿æ—¶è®°å¿†æ•°é‡")
    short_term_messages: int = Field(..., description="çŸ­æ—¶è®°å¿†æ¶ˆæ¯æ•°é‡")


class SummarizeRequest(BaseModel):
    text: str = Field(..., description="éœ€è¦æ€»ç»“çš„æ–‡æœ¬")
    max_length: Optional[int] = Field(15, description="æ€»ç»“çš„æœ€å¤§é•¿åº¦ï¼ˆå­—æ•°ï¼‰")


class SummarizeResponse(BaseModel):
    summary: str = Field(..., description="æ€»ç»“ç»“æœ")


class ExtractRequest(BaseModel):
    text: str = Field(..., description="éœ€è¦æå–ä¿¡æ¯çš„æ–‡æœ¬")


class ExtractResponse(BaseModel):
    extracted_info: str = Field(..., description="æå–çš„å…³é”®ä¿¡æ¯")


class TranslateRequest(BaseModel):
    text: str = Field(..., description="éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬")
    target_language: str = Field(default="English", description="ç›®æ ‡è¯­è¨€")


class TranslateResponse(BaseModel):
    translated_text: str = Field(..., description="ç¿»è¯‘åçš„æ–‡æœ¬")


class SuccessResponse(BaseModel):
    success: bool = Field(..., description="æ“ä½œæ˜¯å¦æˆåŠŸ")
    message: str = Field(default="", description="æ¶ˆæ¯")


# API è·¯ç”±
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "service": "chatbot-agent"}


@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    å¯¹è¯æ¥å£
    
    æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯ï¼Œè¿”å›åŠ©æ‰‹å›å¤
    æ”¯æŒ enable_web_search å‚æ•°å¼ºåˆ¶å¯ç”¨è”ç½‘æœç´¢
    æ”¯æŒ deep_think å‚æ•°å¯ç”¨ TOT æ·±åº¦æ€è€ƒï¼Œå¹¶è¿”å›æ€è€ƒè¿‡ç¨‹
    """
    global chatbot, langgraph_agent
    
    if USE_LANGGRAPH:
        if langgraph_agent is None:
            raise HTTPException(status_code=503, detail="LangGraph Agent not initialized")
        
        try:
            # å¦‚æœå¯ç”¨è”ç½‘æœç´¢ï¼Œä½¿ç”¨å¸¦æœç´¢çš„æ–¹æ³•
            if request.enable_web_search:
                result = langgraph_agent.chat_with_search(
                    request.message,
                    deep_think=request.deep_think,
                    max_branches=request.thought_branches,
                    max_depth=request.thought_depth
                )
            else:
                result = langgraph_agent.chat(
                    request.message,
                    deep_think=request.deep_think,
                    max_branches=request.thought_branches,
                    max_depth=request.thought_depth
                )
            
            # result ç°åœ¨æ˜¯ dictï¼ŒåŒ…å« response, thinking_process, tot_score, deep_think
            return ChatResponse(
                response=result.get("response", ""),
                session_id=request.session_id,
                thinking_process=result.get("thinking_process", ""),
                tot_score=result.get("tot_score", 0.0),
                deep_think=result.get("deep_think", False)
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    else:
        if chatbot is None:
            raise HTTPException(status_code=503, detail="Chatbot not initialized")
        
        try:
            response = chatbot.chat(request.message)
            return ChatResponse(
                response=response,
                session_id=request.session_id,
                thinking_process="",
                tot_score=0.0,
                deep_think=False
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))


async def generate_sse_events(request: ChatRequest) -> AsyncGenerator[str, None]:
    """
    ç”Ÿæˆ SSE äº‹ä»¶æµ
    ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥ç”Ÿæˆå™¨ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
    """
    global langgraph_agent
    
    if langgraph_agent is None:
        yield f"data: {json.dumps({'type': 'error', 'content': 'Agent not initialized'})}\n\n"
        return
    
    try:
        # é€‰æ‹©æµå¼æ–¹æ³•
        if request.enable_web_search:
            stream_func = lambda: langgraph_agent.chat_with_search_stream(
                request.message,
                deep_think=request.deep_think,
                max_branches=request.thought_branches,
                max_depth=request.thought_depth
            )
        else:
            stream_func = lambda: langgraph_agent.chat_stream(
                request.message,
                deep_think=request.deep_think,
                max_branches=request.thought_branches,
                max_depth=request.thought_depth
            )
        
        # ä½¿ç”¨é˜Ÿåˆ—æ¥ä¼ é€’äº‹ä»¶
        import queue
        event_queue = queue.Queue()
        stream_done = threading.Event()
        
        def run_stream():
            try:
                for event in stream_func():
                    event_queue.put(event)
                event_queue.put(None)  # ç»“æŸä¿¡å·
            except Exception as e:
                event_queue.put({'type': 'error', 'content': str(e)})
                event_queue.put(None)
            finally:
                stream_done.set()
        
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥ç”Ÿæˆå™¨
        loop = asyncio.get_event_loop()
        executor = ThreadPoolExecutor(max_workers=1)
        loop.run_in_executor(executor, run_stream)
        
        # å¼‚æ­¥è¯»å–é˜Ÿåˆ— - ä½¿ç”¨æ›´çŸ­çš„è½®è¯¢é—´éš”å®ç°å®æ—¶è¾“å‡º
        while True:
            try:
                # ä½¿ç”¨é˜»å¡å¼è·å–ï¼Œè¶…æ—¶10msï¼Œå®ç°è¿‘å®æ—¶è¾“å‡º
                event = event_queue.get(timeout=0.01)
                if event is None:
                    yield f"data: {json.dumps({'type': 'done'})}\n\n"
                    return
                event_data = json.dumps(event, ensure_ascii=False)
                yield f"data: {event_data}\n\n"
            except queue.Empty:
                # æ£€æŸ¥æµæ˜¯å¦å·²å®Œæˆ
                if stream_done.is_set() and event_queue.empty():
                    yield f"data: {json.dumps({'type': 'done'})}\n\n"
                    return
                await asyncio.sleep(0.005)  # 5ms è½®è¯¢ï¼Œæ›´æµç•…
        
    except Exception as e:
        error_event = json.dumps({'type': 'error', 'content': str(e)}, ensure_ascii=False)
        yield f"data: {error_event}\n\n"


@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    æµå¼å¯¹è¯æ¥å£ (SSE)
    
    æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯ï¼Œä»¥ Server-Sent Events æµå¼è¿”å›åŠ©æ‰‹å›å¤
    æ”¯æŒ enable_web_search å‚æ•°å¼ºåˆ¶å¯ç”¨è”ç½‘æœç´¢
    æ”¯æŒ deep_think å‚æ•°å¯ç”¨ TOT æ·±åº¦æ€è€ƒï¼Œè¾¹æ€è€ƒè¾¹è¾“å‡º
    """
    if not USE_LANGGRAPH:
        raise HTTPException(status_code=400, detail="Streaming only supported with LangGraph agent")
    
    return StreamingResponse(
        generate_sse_events(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ nginx ç¼“å†²
        }
    )


@app.get("/api/stats", response_model=MemoryStatsResponse)
async def get_stats():
    """
    è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯
    """
    global chatbot, langgraph_agent
    
    if USE_LANGGRAPH:
        if langgraph_agent is None:
            raise HTTPException(status_code=503, detail="LangGraph Agent not initialized")
        
        try:
            stats = langgraph_agent.get_memory_stats()
            return MemoryStatsResponse(
                long_term_memories=stats["long_term_memories"],
                short_term_messages=0  # LangGraphç‰ˆæœ¬æ²¡æœ‰çŸ­æœŸè®°å¿†ç»Ÿè®¡
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    else:
        if chatbot is None:
            raise HTTPException(status_code=503, detail="Chatbot not initialized")
        
        try:
            stats = chatbot.get_memory_stats()
            return MemoryStatsResponse(
                long_term_memories=stats["long_term_memories"],
                short_term_messages=stats["short_term_messages"]
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/memory/clear-short-term", response_model=SuccessResponse)
async def clear_short_term_memory():
    """
    æ¸…é™¤çŸ­æ—¶è®°å¿†
    """
    global chatbot
    
    if chatbot is None:
        raise HTTPException(status_code=503, detail="Chatbot not initialized")
    
    try:
        chatbot.clear_short_term_memory()
        return SuccessResponse(success=True, message="Short-term memory cleared")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/memory/clear-all", response_model=SuccessResponse)
async def clear_all_memory():
    """
    æ¸…é™¤æ‰€æœ‰è®°å¿†
    """
    global chatbot, langgraph_agent
    
    if USE_LANGGRAPH:
        if langgraph_agent is None:
            raise HTTPException(status_code=503, detail="LangGraph Agent not initialized")
        try:
            langgraph_agent.clear_all_memory()
            return SuccessResponse(success=True, message="All memory cleared")
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    else:
        if chatbot is None:
            raise HTTPException(status_code=503, detail="Chatbot not initialized")
        try:
            chatbot.clear_all_memory()
            return SuccessResponse(success=True, message="All memory cleared")
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/summarize", response_model=SummarizeResponse)
async def summarize_text(request: SummarizeRequest):
    """
    æ–‡æœ¬æ€»ç»“æ¥å£
    
    å¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œæ™ºèƒ½æ€»ç»“
    """
    global chatbot, langgraph_agent
    
    if USE_LANGGRAPH:
        if langgraph_agent is None:
            raise HTTPException(status_code=503, detail="LangGraph Agent not initialized")
        try:
            summary = langgraph_agent.summarize(request.text, request.max_length)
            return SummarizeResponse(summary=summary)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    else:
        if chatbot is None:
            raise HTTPException(status_code=503, detail="Chatbot not initialized")
        try:
            summary = chatbot.summarize(request.text, request.max_length)
            return SummarizeResponse(summary=summary)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/extract", response_model=ExtractResponse)
async def extract_information(request: ExtractRequest):
    """
    ä¿¡æ¯æå–æ¥å£
    
    ä»æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯
    """
    global chatbot, langgraph_agent
    
    if USE_LANGGRAPH:
        if langgraph_agent is None:
            raise HTTPException(status_code=503, detail="LangGraph Agent not initialized")
        try:
            extracted = langgraph_agent.extract_information(request.text)
            return ExtractResponse(extracted_info=extracted)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    else:
        if chatbot is None:
            raise HTTPException(status_code=503, detail="Chatbot not initialized")
        try:
            extracted = chatbot.extract_information(request.text)
            return ExtractResponse(extracted_info=extracted)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/translate", response_model=TranslateResponse)
async def translate_text(request: TranslateRequest):
    """
    ç¿»è¯‘æ¥å£
    
    å°†æ–‡æœ¬ç¿»è¯‘æˆæŒ‡å®šè¯­è¨€
    """
    global chatbot, langgraph_agent
    
    if USE_LANGGRAPH:
        if langgraph_agent is None:
            raise HTTPException(status_code=503, detail="LangGraph Agent not initialized")
        try:
            translated = langgraph_agent.translate(request.text, request.target_language)
            return TranslateResponse(translated_text=translated)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    else:
        if chatbot is None:
            raise HTTPException(status_code=503, detail="Chatbot not initialized")
        try:
            translated = chatbot.translate(request.text, request.target_language)
            return TranslateResponse(translated_text=translated)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))


# æ–‡ä»¶ä¸Šä¼ å“åº”æ¨¡å‹
class FileUploadResponse(BaseModel):
    success: bool
    filename: str = ""
    original_name: str = ""
    filepath: str = ""
    size: int = 0
    error: str = ""


class FileAnalyzeRequest(BaseModel):
    filepath: str = Field(..., description="æ–‡ä»¶è·¯å¾„")
    question: str = Field(default="è¯·åˆ†æè¿™ä¸ªæ–‡ä»¶çš„å†…å®¹", description="å…³äºæ–‡ä»¶çš„é—®é¢˜")


class FileAnalyzeResponse(BaseModel):
    success: bool
    analysis: str = ""
    file_type: str = ""
    error: str = ""


@app.post("/api/upload", response_model=FileUploadResponse)
async def upload_file(file: UploadFile = File(...)):
    """
    æ–‡ä»¶ä¸Šä¼ æ¥å£
    
    ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨å·¥ä½œç©ºé—´
    """
    global file_handler
    
    if file_handler is None:
        raise HTTPException(status_code=503, detail="File handler not initialized")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        
        # ä¿å­˜æ–‡ä»¶
        result = file_handler.save_uploaded_file(file.filename, content)
        
        if result["success"]:
            return FileUploadResponse(
                success=True,
                filename=result["filename"],
                original_name=result["original_name"],
                filepath=result["filepath"],
                size=result["size"]
            )
        else:
            return FileUploadResponse(
                success=False,
                error=result.get("error", "ä¸Šä¼ å¤±è´¥")
            )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/analyze-file", response_model=FileAnalyzeResponse)
async def analyze_file(request: FileAnalyzeRequest):
    """
    åˆ†æä¸Šä¼ çš„æ–‡ä»¶
    
    è¯»å–æ–‡ä»¶å†…å®¹å¹¶ä½¿ç”¨AIè¿›è¡Œåˆ†æ
    """
    global file_handler, langgraph_agent, chatbot
    
    if file_handler is None:
        raise HTTPException(status_code=503, detail="File handler not initialized")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_result = file_handler.read_uploaded_file_content(request.filepath)
        
        if not file_result["success"]:
            return FileAnalyzeResponse(
                success=False,
                error=file_result.get("error", "è¯»å–æ–‡ä»¶å¤±è´¥")
            )
        
        file_content = file_result["content"]
        file_type = file_result.get("file_type", "unknown")
        
        # æ„å»ºåˆ†ææç¤º
        analysis_prompt = f"""ç”¨æˆ·ä¸Šä¼ äº†ä¸€ä¸ªæ–‡ä»¶ï¼Œè¯·æ ¹æ®æ–‡ä»¶å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ–‡ä»¶ç±»å‹: {file_type}
æ–‡ä»¶å†…å®¹:
{file_content[:5000]}{"...(å†…å®¹å·²æˆªæ–­)" if len(file_content) > 5000 else ""}

ç”¨æˆ·é—®é¢˜: {request.question}

è¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œå›ç­”ã€‚"""
        
        # ä½¿ç”¨AIåˆ†æ
        if USE_LANGGRAPH and langgraph_agent:
            analysis = langgraph_agent.chat(analysis_prompt)
        elif chatbot:
            analysis = chatbot.chat(analysis_prompt)
        else:
            raise HTTPException(status_code=503, detail="No agent available")
        
        return FileAnalyzeResponse(
            success=True,
            analysis=analysis,
            file_type=file_type
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/files")
async def list_uploaded_files():
    """
    åˆ—å‡ºå·²ä¸Šä¼ çš„æ–‡ä»¶
    """
    global file_handler
    
    if file_handler is None:
        raise HTTPException(status_code=503, detail="File handler not initialized")
    
    try:
        result = file_handler.list_files("uploads")
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def run_stdio_mode(args):
    """è¿è¡Œ LangGraph STDIO æ¨¡å¼ï¼ˆçº¯å‘½ä»¤è¡Œï¼Œä¸å¯åŠ¨HTTPæœåŠ¡ï¼‰"""
    print("ğŸš€ å¯åŠ¨ LangGraph STDIO æ¨¡å¼...")
    
    agent = LangGraphAgent(
        model=args.model,
        memory_dir=args.memory,
        workspace_dir=args.workspace,
        default_branches=args.branches,
        default_depth=args.depth,
    )
    
    sys.stdout.write("LangGraph STDIO ready. Type your message and press Enter.\n")
    sys.stdout.flush()
    
    for line in sys.stdin:
        message = line.strip()
        if not message:
            continue
        if message.lower() in ('exit', 'quit', 'q'):
            print("Goodbye!")
            break
        try:
            reply = agent.chat(
                message,
                deep_think=args.deep,
                max_branches=args.branches,
                max_depth=args.depth,
            )
        except Exception as exc:
            reply = f"error: {exc}"
        sys.stdout.write(reply + "\n")
        sys.stdout.flush()


def run_hybrid_mode(args):
    """
    æ··åˆæ¨¡å¼ï¼šåŒæ—¶è¿è¡Œ HTTP API æœåŠ¡å’Œ STDIO äº¤äº’
    - HTTP API åœ¨åå°çº¿ç¨‹è¿è¡Œï¼Œä¾› Java åç«¯è°ƒç”¨
    - STDIO åœ¨ä¸»çº¿ç¨‹è¿è¡Œï¼Œå¯ä»¥ç›´æ¥å‘½ä»¤è¡Œäº¤äº’
    """
    import threading
    import time
    
    print("ğŸš€ å¯åŠ¨æ··åˆæ¨¡å¼ (HTTP API + STDIO)...")
    print(f"   HTTP API: http://localhost:{args.port}")
    print(f"   æ·±åº¦æ€è€ƒ: {'å¼€å¯' if args.deep else 'å…³é—­'}")
    
    # åœ¨åå°çº¿ç¨‹å¯åŠ¨ HTTP æœåŠ¡
    def start_api():
        uvicorn.run(
            "main:app",
            host="0.0.0.0",
            port=args.port,
            reload=False,  # æ··åˆæ¨¡å¼ä¸‹ä¸èƒ½ç”¨ reload
            log_level="warning"  # å‡å°‘æ—¥å¿—å¹²æ‰°
        )
    
    api_thread = threading.Thread(target=start_api, daemon=True)
    api_thread.start()
    
    # ç­‰å¾… API å¯åŠ¨
    time.sleep(3)
    print(f"\nâœ… HTTP API å·²åœ¨åå°è¿è¡Œ (ç«¯å£ {args.port})")
    print("ğŸ’¬ STDIO äº¤äº’å·²å°±ç»ªï¼Œè¾“å…¥æ¶ˆæ¯åå›è½¦å‘é€ï¼Œè¾“å…¥ 'quit' é€€å‡º\n")
    
    # ä½¿ç”¨å…¨å±€çš„ langgraph_agentï¼ˆç”± FastAPI lifespan åˆå§‹åŒ–ï¼‰
    # ä½†è¿™é‡Œéœ€è¦å•ç‹¬åˆ›å»ºä¸€ä¸ªï¼Œå› ä¸º lifespan åœ¨å¦ä¸€ä¸ªçº¿ç¨‹
    agent = LangGraphAgent(
        model=args.model,
        memory_dir=args.memory,
        workspace_dir=args.workspace,
        default_branches=args.branches,
        default_depth=args.depth,
    )
    
    # STDIO äº¤äº’å¾ªç¯
    try:
        while True:
            try:
                message = input("You: ").strip()
            except EOFError:
                break
            
            if not message:
                continue
            if message.lower() in ('exit', 'quit', 'q'):
                print("Goodbye!")
                break
            
            try:
                reply = agent.chat(
                    message,
                    deep_think=args.deep,
                    max_branches=args.branches,
                    max_depth=args.depth,
                )
                print(f"Agent: {reply}\n")
            except Exception as exc:
                print(f"Error: {exc}\n")
    except KeyboardInterrupt:
        print("\nGoodbye!")


def run_api_mode(port: int):
    """è¿è¡Œ FastAPI HTTP æ¨¡å¼"""
    print(f"ğŸš€ å¯åŠ¨ FastAPI HTTP æ¨¡å¼ï¼Œç«¯å£: {port}")
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=True
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Chatbot Agent - æ”¯æŒ HTTP APIã€STDIO å’Œæ··åˆæ¨¡å¼"
    )
    parser.add_argument(
        "--stdio", 
        action="store_true", 
        help="çº¯ STDIO æ¨¡å¼ï¼ˆä¸å¯åŠ¨ HTTP æœåŠ¡ï¼‰"
    )
    parser.add_argument(
        "--hybrid", 
        action="store_true", 
        help="æ··åˆæ¨¡å¼ï¼šåŒæ—¶è¿è¡Œ HTTP API å’Œ STDIO äº¤äº’"
    )
    parser.add_argument(
        "--port", 
        type=int, 
        default=int(os.getenv("AGENT_PORT", "8000")), 
        help="HTTP API ç«¯å£ (é»˜è®¤: 8000)"
    )
    parser.add_argument(
        "--deep", 
        action="store_true", 
        help="å¯ç”¨æ·±åº¦æ€è€ƒ (Tree-of-Thought)"
    )
    parser.add_argument(
        "--branches", 
        type=int, 
        default=5, 
        help="æ€è€ƒåˆ†æ”¯æ•°é‡ (é»˜è®¤: 5)"
    )
    parser.add_argument(
        "--depth", 
        type=int, 
        default=3, 
        help="æ€è€ƒæ·±åº¦ (é»˜è®¤: 3)"
    )
    parser.add_argument(
        "--model", 
        default="deepseek-chat", 
        help="æ¨¡å‹åç§° (é»˜è®¤: deepseek-chat)"
    )
    parser.add_argument(
        "--workspace", 
        default="./workspace", 
        help="å·¥ä½œåŒºç›®å½• (é»˜è®¤: ./workspace)"
    )
    parser.add_argument(
        "--memory", 
        default="./chat_memory_db", 
        help="è®°å¿†å­˜å‚¨ç›®å½• (é»˜è®¤: ./chat_memory_db)"
    )
    
    args = parser.parse_args()
    
    if args.stdio:
        run_stdio_mode(args)
    elif args.hybrid:
        run_hybrid_mode(args)
    else:
        run_api_mode(args.port)

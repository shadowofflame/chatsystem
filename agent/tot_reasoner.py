"""
Tree-of-Thoughts style deep reasoning helper.
Generates multiple thought branches and scores them to pick the best answer.
æ”¯æŒæµå¼è¾“å‡ºï¼Œè¾¹æ€è€ƒè¾¹è¾“å‡ºã€‚
"""

import json
from dataclasses import dataclass
from typing import List, Optional, Tuple, Callable, Generator, Any

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI


# æµå¼äº‹ä»¶ç±»å‹
class StreamEvent:
    """æµå¼äº‹ä»¶"""
    THINKING_START = "thinking_start"      # å¼€å§‹æ€è€ƒ
    THINKING_STEP = "thinking_step"        # æ€è€ƒæ­¥éª¤
    THINKING_SCORE = "thinking_score"      # è¯„åˆ†ç»“æœ
    THINKING_LAYER = "thinking_layer"      # å±‚çº§ä¿¡æ¯
    THINKING_BEST = "thinking_best"        # æœ€ä½³è·¯å¾„
    THINKING_END = "thinking_end"          # æ€è€ƒç»“æŸ
    RESPONSE_CHUNK = "response_chunk"      # å“åº”ç‰‡æ®µ
    RESPONSE_END = "response_end"          # å“åº”ç»“æŸ
    ERROR = "error"                        # é”™è¯¯


@dataclass
class Thought:
    content: str
    score: float
    path: List[str]


class TreeOfThoughtReasoner:
    """Lightweight Tree-of-Thought reasoning helper with streaming support."""

    def __init__(
        self,
        llm: ChatOpenAI,
        default_branches: int = 5,
        default_depth: int = 3,
    ) -> None:
        self.llm = llm
        self.default_branches = max(1, default_branches)
        self.default_depth = max(1, default_depth)

        self._propose_chain = (
            ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        "ä½ æ˜¯æ·±åº¦æ¨ç†åŠ©æ‰‹ï¼Œä½¿ç”¨åˆ†æ”¯æ€è€ƒï¼ˆTree-of-Thoughtï¼‰ã€‚\n"
                        "ç»™å®šé—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œæå‡ºæœ€å¤š{branches}ä¸ªä¸‹ä¸€æ­¥æ€è·¯ï¼Œç”¨ç®€æ´ä¸­æ–‡è¡¨è¿°ã€‚\n"
                        "è¿”å› JSON æ•°ç»„å­—ç¬¦ä¸²ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»£è¡¨ä¸€ä¸ªå€™é€‰æ€è·¯ã€‚",
                    ),
                    (
                        "human",
                        "é—®é¢˜: {problem}\nä¸Šä¸‹æ–‡: {context}\nå½“å‰è·¯å¾„: {path}\nè¯·ç»™å‡ºä¸‹ä¸€æ­¥å€™é€‰æ€è·¯",
                    ),
                ]
            )
            | self.llm
            | StrOutputParser()
        )

        self._score_chain = (
            ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        "ä½ æ˜¯è¯„ä¼°å‘˜ï¼Œç»™æ€è·¯æ‰“åˆ†ï¼Œ0-10ï¼Œ10æœ€å¥½ã€‚\n"
                        "è¿”å› JSON: {{\"score\": number, \"reason\": string}}ã€‚",
                    ),
                    (
                        "human",
                        "é—®é¢˜: {problem}\nä¸Šä¸‹æ–‡: {context}\nå€™é€‰æ€è·¯: {thought}\nè¯·æ‰“åˆ†å¹¶ç®€è¿°ç†ç”±",
                    ),
                ]
            )
            | self.llm
            | StrOutputParser()
        )

    def _propose(self, problem: str, context: str, path: List[str], branches: int) -> List[str]:
        raw = self._propose_chain.invoke(
            {"problem": problem, "context": context, "path": " -> ".join(path) or "(root)", "branches": branches}
        )
        try:
            data = json.loads(raw)
            if isinstance(data, list):
                return [str(item) for item in data][:branches]
        except Exception:
            pass
        return [line.strip("- ") for line in raw.splitlines() if line.strip()][:branches]

    def _score(self, problem: str, context: str, thought: str) -> Tuple[float, str]:
        raw = self._score_chain.invoke({"problem": problem, "context": context, "thought": thought})
        try:
            data = json.loads(raw)
            score = float(data.get("score", 0))
            reason = str(data.get("reason", ""))
            return score, reason
        except Exception:
            try:
                score_line = raw.strip().split()[0]
                score = float(score_line)
                return score, raw
            except Exception:
                return 0.0, raw

    def solve(
        self,
        problem: str,
        context: str = "",
        max_branches: Optional[int] = None,
        max_depth: Optional[int] = None,
    ) -> dict:
        """Run a small tree search and return the best reasoning path.
        
        Returns:
            dict: {
                "thinking_process": str,  # æ€è€ƒè¿‡ç¨‹
                "best_score": float,      # æœ€ä½³å¾—åˆ†
                "final_answer": str,      # æœ€ç»ˆç­”æ¡ˆ
                "success": bool           # æ˜¯å¦æˆåŠŸ
            }
        """
        # éæµå¼ç‰ˆæœ¬ï¼šæ”¶é›†æ‰€æœ‰äº‹ä»¶ç„¶åè¿”å›
        thinking_steps = []
        best_score = 0.0
        final_answer = ""
        success = False
        
        for event in self.solve_stream(problem, context, max_branches, max_depth):
            event_type = event.get("type", "")
            if event_type in [StreamEvent.THINKING_START, StreamEvent.THINKING_LAYER, 
                              StreamEvent.THINKING_STEP, StreamEvent.THINKING_SCORE,
                              StreamEvent.THINKING_BEST]:
                thinking_steps.append(event.get("content", ""))
            elif event_type == StreamEvent.THINKING_END:
                best_score = event.get("best_score", 0.0)
                final_answer = event.get("final_answer", "")
                success = event.get("success", False)
        
        return {
            "thinking_process": "\n".join(thinking_steps),
            "best_score": best_score,
            "final_answer": final_answer,
            "success": success
        }
    
    def solve_stream(
        self,
        problem: str,
        context: str = "",
        max_branches: Optional[int] = None,
        max_depth: Optional[int] = None,
    ) -> Generator[dict, None, None]:
        """
        æµå¼ç‰ˆæœ¬çš„ solveï¼Œè¾¹æ€è€ƒè¾¹è¾“å‡ºäº‹ä»¶ã€‚
        
        Yields:
            dict: åŒ…å« type å’Œ content çš„äº‹ä»¶å­—å…¸
        """
        branches = max_branches or self.default_branches
        depth_limit = max_depth or self.default_depth

        # å¼€å§‹äº‹ä»¶
        yield {
            "type": StreamEvent.THINKING_START,
            "content": f"ğŸ¯ é—®é¢˜: {problem}\nâš™ï¸ å‚æ•°: åˆ†æ”¯æ•°={branches}, æ·±åº¦={depth_limit}"
        }

        frontier: List[Thought] = [Thought(content=problem, score=0.0, path=[problem])]
        best: Optional[Thought] = None

        for depth in range(depth_limit):
            yield {
                "type": StreamEvent.THINKING_LAYER,
                "content": f"ğŸ“Š ç¬¬ {depth + 1}/{depth_limit} å±‚æ¢ç´¢...",
                "layer": depth + 1,
                "total_layers": depth_limit
            }
            
            next_frontier: List[Thought] = []
            for node in frontier:
                proposals = self._propose(problem, context, node.path, branches)
                
                current_path = ' â†’ '.join(node.path[-2:]) if len(node.path) > 1 else '(èµ·ç‚¹)'
                yield {
                    "type": StreamEvent.THINKING_STEP,
                    "content": f"  â””â”€ å½“å‰è·¯å¾„: {current_path}",
                    "path": current_path
                }
                
                for i, proposal in enumerate(proposals, 1):
                    score, reason = self._score(problem, context, proposal)
                    thought_path = node.path + [proposal]
                    combined = f"æ€è·¯: {proposal}\nç†ç”±: {reason}"
                    candidate = Thought(content=combined, score=score, path=thought_path)
                    next_frontier.append(candidate)
                    
                    # è¾“å‡ºè¯„åˆ†äº‹ä»¶
                    short_proposal = proposal[:50] + ('...' if len(proposal) > 50 else '')
                    yield {
                        "type": StreamEvent.THINKING_SCORE,
                        "content": f"     {i}. [{score:.1f}åˆ†] {short_proposal}",
                        "proposal": proposal,
                        "score": score,
                        "reason": reason
                    }
                    
                    if best is None or score > best.score:
                        best = candidate
            
            frontier = sorted(next_frontier, key=lambda t: t.score, reverse=True)[:branches]
            yield {
                "type": StreamEvent.THINKING_STEP,
                "content": f"  âœ… ä¿ç•™å‰ {len(frontier)} ä¸ªæœ€ä¼˜æ€è·¯"
            }
            
            if not frontier:
                break

        if best is None:
            yield {
                "type": StreamEvent.THINKING_END,
                "content": "âŒ æœªèƒ½ç”Ÿæˆæœ‰æ•ˆæ€è·¯",
                "best_score": 0.0,
                "final_answer": "æœªèƒ½ç”Ÿæˆæœ‰æ•ˆæ€è·¯ï¼Œè¯·å°è¯•æä¾›æ›´å¤šä¿¡æ¯ã€‚",
                "success": False
            }
            return

        # è¾“å‡ºæœ€ä½³è·¯å¾„
        best_path_text = "\n".join(f"  Step {i+1}: {item}" for i, item in enumerate(best.path[1:]))
        yield {
            "type": StreamEvent.THINKING_BEST,
            "content": f"ğŸ† æœ€ä½³æ¨ç†è·¯å¾„:\n{best_path_text or '  (æ— )'}\nğŸ’¯ æœ€ç»ˆå¾—åˆ†: {best.score:.2f}",
            "best_path": best.path,
            "best_score": best.score
        }
        
        yield {
            "type": StreamEvent.THINKING_END,
            "content": "âœ… æ·±åº¦æ€è€ƒå®Œæˆ",
            "best_score": best.score,
            "final_answer": best.content,
            "success": True
        }

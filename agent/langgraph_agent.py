"""
LangGraph Agent - æ™ºèƒ½å¯¹è¯ä»£ç†
ä½¿ç”¨ LangGraph å®ç°çŠ¶æ€æœºæ¶æ„ï¼Œæ”¯æŒå¤šç§å·¥å…·è°ƒç”¨
"""

import json
from typing import TypedDict, Annotated, Sequence, Literal, Generator
from datetime import datetime

from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser

from tools import FileHandler, WebSearcher, Calculator
from memory_store import MemoryStore
from tot_reasoner import TreeOfThoughtReasoner, StreamEvent


class AgentState(TypedDict):
    """AgentçŠ¶æ€å®šä¹‰"""
    messages: Annotated[Sequence[BaseMessage], "å¯¹è¯æ¶ˆæ¯åˆ—è¡¨"]
    user_input: str  # ç”¨æˆ·è¾“å…¥
    next_action: str  # ä¸‹ä¸€æ­¥åŠ¨ä½œ: chat, search, file_operation, calculate, end
    tool_calls: list  # å·¥å…·è°ƒç”¨åˆ—è¡¨
    tool_results: list  # å·¥å…·ç»“æœåˆ—è¡¨
    memory_context: str  # è®°å¿†ä¸Šä¸‹æ–‡
    final_response: str  # æœ€ç»ˆå“åº”
    thinking_process: str  # TOT æ€è€ƒè¿‡ç¨‹
    tot_score: float  # TOT æœ€ä½³å¾—åˆ†
    needs_web_search: bool  # æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
    needs_file_operation: bool  # æ˜¯å¦éœ€è¦æ–‡ä»¶æ“ä½œ
    needs_calculation: bool  # æ˜¯å¦éœ€è¦è®¡ç®—
    deep_think: bool  # æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒ(TOT)
    thought_branches: int  # åˆ†æ”¯æ•°é‡
    thought_depth: int  # æ·±åº¦


class LangGraphAgent:
    """
    åŸºäº LangGraph çš„æ™ºèƒ½å¯¹è¯ä»£ç†
    
    å·¥ä½œæµç¨‹:
    1. æ¥æ”¶ç”¨æˆ·è¾“å…¥
    2. åˆ†ææ„å›¾ (è·¯ç”±èŠ‚ç‚¹)
    3. æ‰§è¡Œç›¸åº”æ“ä½œ:
       - ç½‘ç»œæœç´¢
       - æ–‡ä»¶æ“ä½œ
       - æ•°å­¦è®¡ç®—
       - æ™®é€šå¯¹è¯
    4. æ•´åˆç»“æœå¹¶è¿”å›
    """
    
    def __init__(
        self,
        api_key: str = None,
        base_url: str = None,
        model: str = "deepseek-chat",
        memory_dir: str = "./memory_db",
        workspace_dir: str = "./workspace",
        default_branches: int = 5,
        default_depth: int = 3
    ):
        """
        åˆå§‹åŒ– LangGraph Agent
        
        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
            memory_dir: è®°å¿†å­˜å‚¨ç›®å½•
            workspace_dir: å·¥ä½œç©ºé—´ç›®å½•
        """
        import os
        api_key = api_key or os.getenv("OPENAI_API_KEY")
        base_url = base_url or os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com")
        
        # åˆå§‹åŒ– LLM
        self.llm = ChatOpenAI(
            model=model,
            api_key=api_key,
            base_url=base_url,
            temperature=0.7,
            max_tokens=2000
        )
        
        # åˆå§‹åŒ–å·¥å…·
        self.file_handler = FileHandler(workspace_dir)
        self.web_searcher = WebSearcher()
        self.calculator = Calculator()
        self.tot_reasoner = TreeOfThoughtReasoner(
            llm=self.llm,
            default_branches=default_branches,
            default_depth=default_depth
        )
        
        # åˆå§‹åŒ–è®°å¿†
        self.memory_store = MemoryStore(persist_directory=memory_dir)
        
        # æ„å»ºçŠ¶æ€å›¾
        self.graph = self._build_graph()
        self.app = self.graph.compile()
    
    def _build_graph(self) -> StateGraph:
        """
        æ„å»º LangGraph çŠ¶æ€å›¾
        
        æµç¨‹:
        1. å…¥å£èŠ‚ç‚¹ check_deep_think åˆ¤æ–­æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒ
        2. å¦‚æœå¯ç”¨æ·±åº¦æ€è€ƒ â†’ ç›´æ¥è¿›å…¥ deep_think_flowï¼ˆæ£€ç´¢è®°å¿† + TOTï¼‰
        3. å¦‚æœæ™®é€šæ¨¡å¼ â†’ èµ° analyze_intent æ„å›¾åˆ†ææµç¨‹
        """
        
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("check_deep_think", self._check_deep_think)  # å…¥å£ï¼šæ£€æŸ¥æ˜¯å¦æ·±åº¦æ€è€ƒ
        workflow.add_node("retrieve_memory_for_tot", self._retrieve_memory)  # æ·±åº¦æ€è€ƒå‰çš„è®°å¿†æ£€ç´¢
        workflow.add_node("deep_think", self._deep_think)  # æ·±åº¦æ€è€ƒ(TOT)
        workflow.add_node("analyze_intent", self._analyze_intent)  # æ„å›¾åˆ†æï¼ˆæ™®é€šæ¨¡å¼ï¼‰
        workflow.add_node("retrieve_memory", self._retrieve_memory)  # æ£€ç´¢è®°å¿†
        workflow.add_node("web_search", self._web_search)  # ç½‘ç»œæœç´¢
        workflow.add_node("file_operation", self._file_operation)  # æ–‡ä»¶æ“ä½œ
        workflow.add_node("calculate", self._calculate)  # è®¡ç®—
        workflow.add_node("generate_response", self._generate_response)  # ç”Ÿæˆå“åº”
        workflow.add_node("save_memory", self._save_memory)  # ä¿å­˜è®°å¿†
        
        # è®¾ç½®å…¥å£ï¼šé¦–å…ˆæ£€æŸ¥æ˜¯å¦æ·±åº¦æ€è€ƒ
        workflow.set_entry_point("check_deep_think")
        
        # å…¥å£è·¯ç”±ï¼šæ·±åº¦æ€è€ƒ vs æ™®é€šæ¨¡å¼
        workflow.add_conditional_edges(
            "check_deep_think",
            self._route_entry,
            {
                "deep_think": "retrieve_memory_for_tot",  # æ·±åº¦æ€è€ƒï¼šå…ˆæ£€ç´¢è®°å¿†
                "normal": "analyze_intent"  # æ™®é€šæ¨¡å¼ï¼šèµ°æ„å›¾åˆ†æ
            }
        )
        
        # æ·±åº¦æ€è€ƒæµç¨‹ï¼šæ£€ç´¢è®°å¿† â†’ TOT â†’ ä¿å­˜
        workflow.add_edge("retrieve_memory_for_tot", "deep_think")
        workflow.add_edge("deep_think", "save_memory")
        
        # æ™®é€šæ¨¡å¼ï¼šæ„å›¾åˆ†æåè·¯ç”±
        workflow.add_conditional_edges(
            "analyze_intent",
            self._route_decision,
            {
                "memory": "retrieve_memory",
                "search": "web_search",
                "file": "file_operation",
                "calculate": "calculate",
                "chat": "retrieve_memory"
            }
        )
        
        # æ™®é€šæ¨¡å¼å„èŠ‚ç‚¹ â†’ ç”Ÿæˆå“åº”
        workflow.add_edge("retrieve_memory", "generate_response")
        workflow.add_edge("web_search", "generate_response")
        workflow.add_edge("file_operation", "generate_response")
        workflow.add_edge("calculate", "generate_response")
        
        # ç”Ÿæˆå“åº” â†’ ä¿å­˜è®°å¿†
        workflow.add_edge("generate_response", "save_memory")
        
        # ä¿å­˜è®°å¿† â†’ ç»“æŸ
        workflow.add_edge("save_memory", END)
        
        return workflow
    
    def _check_deep_think(self, state: AgentState) -> AgentState:
        """å…¥å£èŠ‚ç‚¹ï¼šæ£€æŸ¥æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒ"""
        deep_think = state.get("deep_think", False)
        if deep_think:
            print("ğŸ§  æ£€æµ‹åˆ°æ·±åº¦æ€è€ƒæ¨¡å¼ï¼Œå°†ä½¿ç”¨ Tree-of-Thought æ¨ç†")
        else:
            print("ğŸ’¬ æ™®é€šå¯¹è¯æ¨¡å¼")
        return state
    
    def _route_entry(self, state: AgentState) -> Literal["deep_think", "normal"]:
        """å…¥å£è·¯ç”±ï¼šæ ¹æ® deep_think å‚æ•°å†³å®šèµ°å“ªæ¡è·¯å¾„"""
        if state.get("deep_think", False):
            return "deep_think"
        return "normal"
    
    def _analyze_intent(self, state: AgentState) -> AgentState:
        """
        åˆ†æç”¨æˆ·æ„å›¾
        
        åˆ¤æ–­ç”¨æˆ·éœ€è¦:
        - ç½‘ç»œæœç´¢ (æœ€æ–°ä¿¡æ¯ã€æ–°é—»ã€å®æ—¶æ•°æ®)
        - æ–‡ä»¶æ“ä½œ (è¯»å†™æ–‡ä»¶ã€æŸ¥çœ‹ç›®å½•)
        - è®¡ç®— (æ•°å­¦è®¡ç®—ã€æ•°æ®å¤„ç†)
        - æ™®é€šå¯¹è¯
        """
        user_input = state["user_input"]
        
        # ä½¿ç”¨ LLM åˆ†ææ„å›¾
        intent_prompt = f"""åˆ†æç”¨æˆ·çš„æ„å›¾ï¼Œåˆ¤æ–­éœ€è¦æ‰§è¡Œä»€ä¹ˆæ“ä½œã€‚

ç”¨æˆ·è¾“å…¥: {user_input}

è¯·åˆ¤æ–­ç”¨æˆ·çš„æ„å›¾å¹¶è¿”å›JSONæ ¼å¼:
{{
    "intent": "search|file|calculate|chat",
    "reason": "åˆ¤æ–­ç†ç”±",
    "needs_web_search": true/false,
    "needs_file_operation": true/false,
    "needs_calculation": true/false
}}

åˆ¤æ–­æ ‡å‡†:
- search: éœ€è¦æœ€æ–°ä¿¡æ¯ã€æ–°é—»ã€å®æ—¶æ•°æ®ã€å¤©æ°”ç­‰
- file: æ¶‰åŠè¯»å†™æ–‡ä»¶ã€æŸ¥çœ‹ç›®å½•ã€ä¿å­˜å†…å®¹ç­‰
- calculate: éœ€è¦æ•°å­¦è®¡ç®—ã€æ•°æ®å¤„ç†
- chat: æ™®é€šå¯¹è¯ã€å›ç­”çŸ¥è¯†æ€§é—®é¢˜

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            response = self.llm.invoke([HumanMessage(content=intent_prompt)])
            intent_data = json.loads(response.content)
            
            state["next_action"] = intent_data.get("intent", "chat")
            state["needs_web_search"] = intent_data.get("needs_web_search", False)
            state["needs_file_operation"] = intent_data.get("needs_file_operation", False)
            state["needs_calculation"] = intent_data.get("needs_calculation", False)
            
            print(f"ğŸ” æ„å›¾åˆ†æ: {intent_data.get('intent')} - {intent_data.get('reason')}")
            
        except Exception as e:
            print(f"âš ï¸ æ„å›¾åˆ†æå¤±è´¥ï¼Œé»˜è®¤ä¸ºæ™®é€šå¯¹è¯: {e}")
            state["next_action"] = "chat"
            state["needs_web_search"] = False
            state["needs_file_operation"] = False
            state["needs_calculation"] = False
        
        return state
    
    def _route_decision(self, state: AgentState) -> Literal["memory", "search", "file", "calculate", "chat"]:
        """è·¯ç”±å†³ç­–"""
        if state.get("needs_web_search"):
            return "search"
        elif state.get("needs_file_operation"):
            return "file"
        elif state.get("needs_calculation"):
            return "calculate"
        else:
            return "memory"
    
    def _retrieve_memory(self, state: AgentState) -> AgentState:
        """æ£€ç´¢ç›¸å…³è®°å¿†"""
        user_input = state["user_input"]
        
        # æ£€ç´¢ç›¸å…³è®°å¿†
        relevant_memories = self.memory_store.search_memories(user_input, n_results=5)
        
        if relevant_memories:
            memory_context = "ã€ç›¸å…³å†å²è®°å¿†ã€‘\n"
            for i, memory in enumerate(relevant_memories, 1):
                memory_context += f"{i}. {memory['content']}\n"
        else:
            memory_context = "ï¼ˆæš‚æ— ç›¸å…³å†å²è®°å¿†ï¼‰"
        
        state["memory_context"] = memory_context
        print(f"ğŸ“š æ£€ç´¢åˆ° {len(relevant_memories)} æ¡ç›¸å…³è®°å¿†")
        
        return state
    
    def _web_search(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œç½‘ç»œæœç´¢"""
        user_input = state["user_input"]
        
        print(f"ğŸŒ æ‰§è¡Œç½‘ç»œæœç´¢: {user_input}")
        search_result = self.web_searcher.search(user_input, num_results=5)
        
        if search_result["success"]:
            results_text = "ã€ç½‘ç»œæœç´¢ç»“æœã€‘\n"
            for i, result in enumerate(search_result["results"], 1):
                results_text += f"{i}. {result['title']}\n"
                results_text += f"   {result['snippet']}\n"
                results_text += f"   æ¥æº: {result['link']}\n\n"
            
            state["tool_results"] = [{"type": "search", "content": results_text}]
            state["memory_context"] = results_text
        else:
            state["tool_results"] = [{"type": "search", "content": f"æœç´¢å¤±è´¥: {search_result.get('error')}"}]
            state["memory_context"] = "æœç´¢æœªèƒ½è¿”å›ç»“æœ"
        
        return state
    
    def _file_operation(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œæ–‡ä»¶æ“ä½œ"""
        user_input = state["user_input"]
        
        print(f"ğŸ“ æ‰§è¡Œæ–‡ä»¶æ“ä½œ")
        
        # ä½¿ç”¨LLMè§£ææ–‡ä»¶æ“ä½œæ„å›¾
        file_prompt = f"""ç”¨æˆ·æƒ³è¦æ‰§è¡Œæ–‡ä»¶æ“ä½œï¼Œè¯·è§£æå…·ä½“æ“ä½œå¹¶è¿”å›JSONæ ¼å¼:

ç”¨æˆ·è¾“å…¥: {user_input}

è¿”å›æ ¼å¼:
{{
    "operation": "read|write|list|delete",
    "filepath": "æ–‡ä»¶è·¯å¾„",
    "content": "å†™å…¥å†…å®¹ï¼ˆä»…writeæ“ä½œéœ€è¦ï¼‰"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            response = self.llm.invoke([HumanMessage(content=file_prompt)])
            file_op = json.loads(response.content)
            
            operation = file_op.get("operation")
            filepath = file_op.get("filepath", "")
            
            if operation == "read":
                result = self.file_handler.read_file(filepath)
            elif operation == "write":
                content = file_op.get("content", "")
                result = self.file_handler.write_file(filepath, content)
            elif operation == "list":
                result = self.file_handler.list_files(filepath or ".")
            elif operation == "delete":
                result = self.file_handler.delete_file(filepath)
            else:
                result = {"success": False, "error": "æœªçŸ¥çš„æ–‡ä»¶æ“ä½œ"}
            
            state["tool_results"] = [{"type": "file", "content": json.dumps(result, ensure_ascii=False, indent=2)}]
            state["memory_context"] = f"æ–‡ä»¶æ“ä½œç»“æœ: {json.dumps(result, ensure_ascii=False)}"
            
        except Exception as e:
            error_msg = f"æ–‡ä»¶æ“ä½œè§£æå¤±è´¥: {str(e)}"
            state["tool_results"] = [{"type": "file", "content": error_msg}]
            state["memory_context"] = error_msg
        
        return state
    
    def _calculate(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œè®¡ç®—"""
        user_input = state["user_input"]
        
        print(f"ğŸ§® æ‰§è¡Œè®¡ç®—")
        
        # ä½¿ç”¨LLMæå–æ•°å­¦è¡¨è¾¾å¼
        calc_prompt = f"""ä»ç”¨æˆ·è¾“å…¥ä¸­æå–æ•°å­¦è¡¨è¾¾å¼å¹¶è¿”å›JSON:

ç”¨æˆ·è¾“å…¥: {user_input}

è¿”å›æ ¼å¼:
{{
    "expression": "æ•°å­¦è¡¨è¾¾å¼ï¼ˆå¦‚: 2 + 2, 3 * 5 + 10ï¼‰"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            response = self.llm.invoke([HumanMessage(content=calc_prompt)])
            calc_op = json.loads(response.content)
            
            expression = calc_op.get("expression", "")
            result = self.calculator.calculate(expression)
            
            state["tool_results"] = [{"type": "calculate", "content": json.dumps(result, ensure_ascii=False)}]
            state["memory_context"] = f"è®¡ç®—ç»“æœ: {result}"
            
        except Exception as e:
            error_msg = f"è®¡ç®—å¤±è´¥: {str(e)}"
            state["tool_results"] = [{"type": "calculate", "content": error_msg}]
            state["memory_context"] = error_msg
        
        return state
    
    def _deep_think(self, state: AgentState) -> AgentState:
        """æ·±åº¦æ€è€ƒèŠ‚ç‚¹ - ä½¿ç”¨ Tree-of-Thought è¿›è¡Œå¤šåˆ†æ”¯æ¨ç†"""
        user_input = state["user_input"]
        memory_context = state.get("memory_context", "")
        tool_results = state.get("tool_results", [])
        thought_branches = state.get("thought_branches", 3)
        thought_depth = state.get("thought_depth", 2)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if memory_context:
            context_parts.append(memory_context)
        if tool_results:
            for result in tool_results:
                context_parts.append(f"\nã€{result['type']}å·¥å…·ç»“æœã€‘\n{result['content']}")
        full_context = "\n".join(context_parts)
        
        print("ğŸ§  æ·±åº¦æ€è€ƒæ¨¡å¼ (Tree-of-Thought)")
        print(f"   åˆ†æ”¯æ•°: {thought_branches}, æ·±åº¦: {thought_depth}")
        
        try:
            # solve ç°åœ¨è¿”å› dictï¼ŒåŒ…å« thinking_process å’Œ final_answer
            tot_result = self.tot_reasoner.solve(
                problem=user_input,
                context=full_context,
                max_branches=thought_branches,
                max_depth=thought_depth
            )
            
            # å°†æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆåˆ†å¼€å­˜å‚¨
            state["thinking_process"] = tot_result.get("thinking_process", "")
            state["final_response"] = tot_result.get("final_answer", "")
            state["tot_score"] = tot_result.get("best_score", 0.0)
            
            print("âœ… æ·±åº¦æ€è€ƒå®Œæˆ")
        except Exception as e:
            state["thinking_process"] = f"æ€è€ƒè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
            state["final_response"] = f"æ·±åº¦æ€è€ƒå¤±è´¥: {str(e)}"
            state["tot_score"] = 0.0
            print(f"âŒ æ·±åº¦æ€è€ƒå¤±è´¥: {e}")
        
        return state
    
    def _generate_response(self, state: AgentState) -> AgentState:
        """ç”Ÿæˆæœ€ç»ˆå“åº”ï¼ˆæ™®é€šæ¨¡å¼ï¼‰"""
        user_input = state["user_input"]
        memory_context = state.get("memory_context", "")
        tool_results = state.get("tool_results", [])
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        
        if memory_context:
            context_parts.append(memory_context)
        
        if tool_results:
            for result in tool_results:
                context_parts.append(f"\nã€{result['type']}å·¥å…·ç»“æœã€‘\n{result['content']}")
        
        full_context = "\n".join(context_parts)
        
        response_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚:
1. å¦‚æœæœ‰æœç´¢ç»“æœï¼ŒåŸºäºæœç´¢ç»“æœå›ç­”
2. å¦‚æœæœ‰æ–‡ä»¶æ“ä½œç»“æœï¼Œè¯´æ˜æ“ä½œç»“æœ
3. å¦‚æœæœ‰è®¡ç®—ç»“æœï¼Œç»™å‡ºè®¡ç®—ç­”æ¡ˆ
4. å›ç­”è¦å‡†ç¡®ã€å‹å¥½ã€æœ‰å¸®åŠ©
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯šå®è¯´æ˜

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}"""),
            ("human", "{input}")
        ])
        
        chain = response_prompt | self.llm | StrOutputParser()
        
        try:
            response = chain.invoke({
                "context": full_context,
                "input": user_input
            })
            
            state["final_response"] = response
            print(f"âœ… ç”Ÿæˆå“åº”å®Œæˆ")
            
        except Exception as e:
            state["final_response"] = f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
        
        return state
    
    def _save_memory(self, state: AgentState) -> AgentState:
        """ä¿å­˜å¯¹è¯åˆ°è®°å¿†"""
        user_input = state["user_input"]
        final_response = state.get("final_response", "")
        
        # ä¿å­˜åˆ°é•¿æœŸè®°å¿†
        self.memory_store.add_memory(user_input, final_response)
        print(f"ğŸ’¾ ä¿å­˜è®°å¿†å®Œæˆ")
        
        return state
    
    def chat(self, user_input: str, deep_think: bool = False, max_branches: int = 5, max_depth: int = 3) -> dict:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            deep_think: æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒ
            max_branches: TOT åˆ†æ”¯æ•°
            max_depth: TOT æ·±åº¦
            
        Returns:
            dict: {
                "response": str,           # æœ€ç»ˆå›ç­”
                "thinking_process": str,   # æ€è€ƒè¿‡ç¨‹ï¼ˆä»…æ·±åº¦æ€è€ƒæ—¶æœ‰å€¼ï¼‰
                "tot_score": float,        # TOT å¾—åˆ†ï¼ˆä»…æ·±åº¦æ€è€ƒæ—¶æœ‰å€¼ï¼‰
                "deep_think": bool         # æ˜¯å¦ä½¿ç”¨äº†æ·±åº¦æ€è€ƒ
            }
        """
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = {
            "messages": [],
            "user_input": user_input,
            "next_action": "",
            "tool_calls": [],
            "tool_results": [],
            "memory_context": "",
            "final_response": "",
            "thinking_process": "",
            "tot_score": 0.0,
            "needs_web_search": False,
            "needs_file_operation": False,
            "needs_calculation": False,
            "deep_think": deep_think,
            "thought_branches": max_branches,
            "thought_depth": max_depth
        }
        
        # è¿è¡ŒçŠ¶æ€å›¾
        print(f"\n{'='*50}")
        print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
        print(f"{'='*50}\n")
        
        final_state = self.app.invoke(initial_state)
        
        return {
            "response": final_state.get("final_response", ""),
            "thinking_process": final_state.get("thinking_process", ""),
            "tot_score": final_state.get("tot_score", 0.0),
            "deep_think": deep_think
        }
    
    def chat_with_search(self, user_input: str, deep_think: bool = False, max_branches: int = 5, max_depth: int = 3) -> dict:
        """
        å¼ºåˆ¶ä½¿ç”¨è”ç½‘æœç´¢å¤„ç†ç”¨æˆ·è¾“å…¥
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            deep_think: æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒ
            max_branches: TOT åˆ†æ”¯æ•°
            max_depth: TOT æ·±åº¦
            
        Returns:
            dict: {
                "response": str,           # æœ€ç»ˆå›ç­”
                "thinking_process": str,   # æ€è€ƒè¿‡ç¨‹ï¼ˆä»…æ·±åº¦æ€è€ƒæ—¶æœ‰å€¼ï¼‰
                "tot_score": float,        # TOT å¾—åˆ†ï¼ˆä»…æ·±åº¦æ€è€ƒæ—¶æœ‰å€¼ï¼‰
                "deep_think": bool         # æ˜¯å¦ä½¿ç”¨äº†æ·±åº¦æ€è€ƒ
            }
        """
        print(f"\n{'='*50}")
        print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
        print(f"ğŸŒ å¼ºåˆ¶è”ç½‘æœç´¢æ¨¡å¼")
        print(f"{'='*50}\n")
        
        # æ‰§è¡Œç½‘ç»œæœç´¢
        print(f"ğŸ” æ‰§è¡Œç½‘ç»œæœç´¢: {user_input}")
        search_result = self.web_searcher.search(user_input, num_results=5)
        
        if search_result["success"]:
            results_text = "ã€ç½‘ç»œæœç´¢ç»“æœã€‘\n"
            for i, result in enumerate(search_result["results"], 1):
                results_text += f"{i}. {result['title']}\n"
                results_text += f"   {result['snippet']}\n"
                results_text += f"   æ¥æº: {result['link']}\n\n"
            print(f"âœ… æœç´¢æˆåŠŸï¼Œè·å– {len(search_result['results'])} æ¡ç»“æœ")
        else:
            results_text = f"æœç´¢æœªèƒ½è¿”å›ç»“æœ: {search_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            print(f"âš ï¸ æœç´¢å¤±è´¥: {search_result.get('error')}")
        
        # æ£€ç´¢ç›¸å…³è®°å¿†
        relevant_memories = self.memory_store.search_memories(user_input, n_results=3)
        memory_context = ""
        if relevant_memories:
            memory_context = "\n\nã€ç›¸å…³å†å²è®°å¿†ã€‘\n"
            for i, memory in enumerate(relevant_memories, 1):
                memory_context += f"{i}. {memory['content']}\n"
        
        if deep_think:
            print("ğŸ§  æ·±åº¦æ€è€ƒæ¨¡å¼ (æœç´¢+TOT)")
            try:
                tot_result = self.tot_reasoner.solve(
                    problem=user_input,
                    context=results_text + memory_context,
                    max_branches=max_branches,
                    max_depth=max_depth
                )
                final_response = tot_result.get("final_answer", "")
                thinking_process = tot_result.get("thinking_process", "")
                tot_score = tot_result.get("best_score", 0.0)
                
                self.memory_store.add_memory(user_input, final_response)
                print("âœ… æ·±åº¦æ€è€ƒå®Œæˆ")
                print("ğŸ’¾ ä¿å­˜è®°å¿†å®Œæˆ")
                
                return {
                    "response": final_response,
                    "thinking_process": thinking_process,
                    "tot_score": tot_score,
                    "deep_think": True
                }
            except Exception as e:
                error_msg = f"æ·±åº¦æ€è€ƒå¤±è´¥: {str(e)}"
                print(f"âŒ é”™è¯¯: {error_msg}")
                return {
                    "response": error_msg,
                    "thinking_process": "",
                    "tot_score": 0.0,
                    "deep_think": True
                }
        else:
            response_prompt = ChatPromptTemplate.from_messages([
                ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨ç½‘ç»œæœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚:
1. åŸºäºæœç´¢ç»“æœå›ç­”é—®é¢˜
2. å¦‚æœ‰å¤šä¸ªæ¥æºï¼Œç»¼åˆä¿¡æ¯å›ç­”
3. é€‚å½“å¼•ç”¨æ¥æº
4. å¦‚æœæœç´¢ç»“æœä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯šå®è¯´æ˜
5. å›ç­”è¦å‡†ç¡®ã€æœ‰å¸®åŠ©

{context}"""),
                ("human", "{input}")
            ])
            
            chain = response_prompt | self.llm | StrOutputParser()
            
            try:
                response = chain.invoke({
                    "context": results_text + memory_context,
                    "input": user_input
                })
                print(f"âœ… ç”Ÿæˆå“åº”å®Œæˆ")
                
                # ä¿å­˜åˆ°é•¿æœŸè®°å¿†
                self.memory_store.add_memory(user_input, response)
                print(f"ğŸ’¾ ä¿å­˜è®°å¿†å®Œæˆ")
                
                return {
                    "response": response,
                    "thinking_process": "",
                    "tot_score": 0.0,
                    "deep_think": False
                }
                
            except Exception as e:
                error_msg = f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
                print(f"âŒ é”™è¯¯: {error_msg}")
                return {
                    "response": error_msg,
                    "thinking_process": "",
                    "tot_score": 0.0,
                    "deep_think": False
                }
    
    def get_memory_stats(self):
        """è·å–è®°å¿†ç»Ÿè®¡"""
        return {
            "long_term_memories": self.memory_store.get_memory_count()
        }
    
    def clear_all_memory(self):
        """æ¸…é™¤æ‰€æœ‰è®°å¿†"""
        self.memory_store.clear_all_memories()
    
    def summarize(self, text: str, max_length: int = None) -> str:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œæ€»ç»“
        
        Args:
            text: éœ€è¦æ€»ç»“çš„æ–‡æœ¬
            max_length: æ€»ç»“çš„æœ€å¤§é•¿åº¦ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ€»ç»“åçš„æ–‡æœ¬
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ€»ç»“åŠ©æ‰‹ã€‚è¯·ç®€æ´ã€å‡†ç¡®åœ°æ€»ç»“ç”¨æˆ·æä¾›çš„æ–‡æœ¬ã€‚"),
            ("human", f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ€»ç»“ï¼š\n\n{text}" + (f"\n\nè¦æ±‚ï¼šæ€»ç»“é•¿åº¦ä¸è¶…è¿‡{max_length}å­—ã€‚" if max_length else ""))
        ])
        
        chain = prompt | self.llm | StrOutputParser()
        
        try:
            return chain.invoke({})
        except Exception as e:
            return f"æ€»ç»“å¤±è´¥: {str(e)}"
    
    def extract_information(self, text: str) -> str:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯
        
        Args:
            text: éœ€è¦æå–ä¿¡æ¯çš„æ–‡æœ¬
            
        Returns:
            æå–çš„å…³é”®ä¿¡æ¯
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·ä»ç”¨æˆ·æä¾›çš„æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬äººç‰©ã€æ—¶é—´ã€åœ°ç‚¹ã€äº‹ä»¶ç­‰é‡è¦å†…å®¹ã€‚"),
            ("human", f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯ï¼š\n\n{text}")
        ])
        
        chain = prompt | self.llm | StrOutputParser()
        
        try:
            return chain.invoke({})
        except Exception as e:
            return f"ä¿¡æ¯æå–å¤±è´¥: {str(e)}"
    
    def translate(self, text: str, target_language: str = "English") -> str:
        """
        ç¿»è¯‘æ–‡æœ¬
        
        Args:
            text: éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬
            target_language: ç›®æ ‡è¯­è¨€
            
        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·æä¾›çš„æ–‡æœ¬å‡†ç¡®ç¿»è¯‘æˆ{target_language}ã€‚åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚"),
            ("human", text)
        ])
        
        chain = prompt | self.llm | StrOutputParser()
        
        try:
            return chain.invoke({})
        except Exception as e:
            return f"ç¿»è¯‘å¤±è´¥: {str(e)}"

    # ==================== æµå¼æ–¹æ³• ====================
    
    def chat_stream(self, user_input: str, deep_think: bool = False, max_branches: int = 5, max_depth: int = 3) -> Generator[dict, None, None]:
        """
        æµå¼å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè¾¹æ€è€ƒè¾¹è¾“å‡º
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            deep_think: æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒ
            max_branches: TOT åˆ†æ”¯æ•°
            max_depth: TOT æ·±åº¦
            
        Yields:
            dict: æµå¼äº‹ä»¶
        """
        yield {"type": "status", "content": "å¼€å§‹å¤„ç†..."}
        
        # æ£€ç´¢ç›¸å…³è®°å¿†
        relevant_memories = self.memory_store.search_memories(user_input, n_results=3)
        memory_context = ""
        if relevant_memories:
            memory_context = "\n\nã€ç›¸å…³å†å²è®°å¿†ã€‘\n"
            for i, memory in enumerate(relevant_memories, 1):
                memory_context += f"{i}. {memory['content']}\n"
            yield {"type": "status", "content": f"æ‰¾åˆ° {len(relevant_memories)} æ¡ç›¸å…³è®°å¿†"}
        
        if deep_think:
            yield {"type": "status", "content": "å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ (Tree-of-Thought)..."}
            
            # æµå¼è¾“å‡ºæ€è€ƒè¿‡ç¨‹
            final_answer = ""
            best_score = 0.0
            
            for event in self.tot_reasoner.solve_stream(
                problem=user_input,
                context=memory_context,
                max_branches=max_branches,
                max_depth=max_depth
            ):
                # è½¬å‘ TOT äº‹ä»¶
                yield event
                
                if event.get("type") == StreamEvent.THINKING_END:
                    final_answer = event.get("final_answer", "")
                    best_score = event.get("best_score", 0.0)
            
            # æµå¼è¾“å‡ºæœ€ç»ˆå“åº”
            yield {"type": StreamEvent.RESPONSE_CHUNK, "content": "\n\n---\n\n**æœ€ç»ˆå›ç­”ï¼š**\n\n"}
            
            # ä½¿ç”¨ LLM æµå¼ç”Ÿæˆæœ€ç»ˆå“åº”
            response_prompt = ChatPromptTemplate.from_messages([
                ("system", """åŸºäºæ·±åº¦æ€è€ƒçš„ç»“æœï¼Œç”Ÿæˆç®€æ´æ¸…æ™°çš„å›ç­”ã€‚
                
æ€è€ƒç»“æœ: {thought}
ç”¨æˆ·é—®é¢˜: {question}

è¯·ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦é‡å¤æ€è€ƒè¿‡ç¨‹ã€‚"""),
                ("human", "{question}")
            ])
            
            chain = response_prompt | self.llm
            
            try:
                for chunk in chain.stream({"thought": final_answer, "question": user_input}):
                    if hasattr(chunk, 'content') and chunk.content:
                        yield {"type": StreamEvent.RESPONSE_CHUNK, "content": chunk.content}
            except Exception as e:
                yield {"type": StreamEvent.ERROR, "content": f"ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}"}
            
            # ä¿å­˜è®°å¿†
            self.memory_store.add_memory(user_input, final_answer)
            
            yield {
                "type": StreamEvent.RESPONSE_END,
                "content": "",
                "tot_score": best_score,
                "deep_think": True
            }
        else:
            # æ™®é€šæ¨¡å¼ï¼šç›´æ¥æµå¼ç”Ÿæˆå“åº”
            yield {"type": "status", "content": "ç”Ÿæˆå›ç­”ä¸­..."}
            
            response_prompt = ChatPromptTemplate.from_messages([
                ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚:
1. å›ç­”è¦å‡†ç¡®ã€å‹å¥½ã€æœ‰å¸®åŠ©
2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯šå®è¯´æ˜

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}"""),
                ("human", "{input}")
            ])
            
            chain = response_prompt | self.llm
            full_response = ""
            
            try:
                for chunk in chain.stream({"context": memory_context, "input": user_input}):
                    if hasattr(chunk, 'content') and chunk.content:
                        full_response += chunk.content
                        yield {"type": StreamEvent.RESPONSE_CHUNK, "content": chunk.content}
                
                # ä¿å­˜è®°å¿†
                self.memory_store.add_memory(user_input, full_response)
                
                yield {
                    "type": StreamEvent.RESPONSE_END,
                    "content": "",
                    "tot_score": 0.0,
                    "deep_think": False
                }
            except Exception as e:
                yield {"type": StreamEvent.ERROR, "content": f"ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}"}

    def chat_with_search_stream(self, user_input: str, deep_think: bool = False, max_branches: int = 5, max_depth: int = 3) -> Generator[dict, None, None]:
        """
        æµå¼å¤„ç†è”ç½‘æœç´¢è¯·æ±‚
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            deep_think: æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒ
            max_branches: TOT åˆ†æ”¯æ•°
            max_depth: TOT æ·±åº¦
            
        Yields:
            dict: æµå¼äº‹ä»¶
        """
        yield {"type": "status", "content": "ğŸŒ å¼€å§‹è”ç½‘æœç´¢..."}
        
        # æ‰§è¡Œç½‘ç»œæœç´¢
        search_result = self.web_searcher.search(user_input, num_results=5)
        
        if search_result["success"]:
            results_text = "ã€ç½‘ç»œæœç´¢ç»“æœã€‘\n"
            for i, result in enumerate(search_result["results"], 1):
                results_text += f"{i}. {result['title']}\n"
                results_text += f"   {result['snippet']}\n"
                results_text += f"   æ¥æº: {result['link']}\n\n"
            yield {"type": "status", "content": f"âœ… æœç´¢æˆåŠŸï¼Œè·å– {len(search_result['results'])} æ¡ç»“æœ"}
        else:
            results_text = f"æœç´¢æœªèƒ½è¿”å›ç»“æœ: {search_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            yield {"type": "status", "content": f"âš ï¸ æœç´¢å¤±è´¥: {search_result.get('error')}"}
        
        # æ£€ç´¢ç›¸å…³è®°å¿†
        relevant_memories = self.memory_store.search_memories(user_input, n_results=3)
        memory_context = ""
        if relevant_memories:
            memory_context = "\n\nã€ç›¸å…³å†å²è®°å¿†ã€‘\n"
            for i, memory in enumerate(relevant_memories, 1):
                memory_context += f"{i}. {memory['content']}\n"
        
        full_context = results_text + memory_context
        
        if deep_think:
            yield {"type": "status", "content": "ğŸ§  å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ (æœç´¢+TOT)..."}
            
            final_answer = ""
            best_score = 0.0
            
            for event in self.tot_reasoner.solve_stream(
                problem=user_input,
                context=full_context,
                max_branches=max_branches,
                max_depth=max_depth
            ):
                yield event
                
                if event.get("type") == StreamEvent.THINKING_END:
                    final_answer = event.get("final_answer", "")
                    best_score = event.get("best_score", 0.0)
            
            yield {"type": StreamEvent.RESPONSE_CHUNK, "content": "\n\n---\n\n**æœ€ç»ˆå›ç­”ï¼š**\n\n"}
            
            # ä½¿ç”¨æœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆå“åº”
            response_prompt = ChatPromptTemplate.from_messages([
                ("system", """åŸºäºç½‘ç»œæœç´¢ç»“æœå’Œæ·±åº¦æ€è€ƒï¼Œç”Ÿæˆå‡†ç¡®çš„å›ç­”ã€‚

æœç´¢ç»“æœ: {search_results}
æ€è€ƒç»“æœ: {thought}
ç”¨æˆ·é—®é¢˜: {question}

è¯·ç»¼åˆä¿¡æ¯å›ç­”ï¼Œé€‚å½“å¼•ç”¨æ¥æºã€‚"""),
                ("human", "{question}")
            ])
            
            chain = response_prompt | self.llm
            
            try:
                for chunk in chain.stream({"search_results": results_text, "thought": final_answer, "question": user_input}):
                    if hasattr(chunk, 'content') and chunk.content:
                        yield {"type": StreamEvent.RESPONSE_CHUNK, "content": chunk.content}
            except Exception as e:
                yield {"type": StreamEvent.ERROR, "content": f"ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}"}
            
            self.memory_store.add_memory(user_input, final_answer)
            
            yield {
                "type": StreamEvent.RESPONSE_END,
                "content": "",
                "tot_score": best_score,
                "deep_think": True
            }
        else:
            yield {"type": "status", "content": "ç”Ÿæˆå›ç­”ä¸­..."}
            
            response_prompt = ChatPromptTemplate.from_messages([
                ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨ç½‘ç»œæœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚:
1. åŸºäºæœç´¢ç»“æœå›ç­”é—®é¢˜
2. å¦‚æœ‰å¤šä¸ªæ¥æºï¼Œç»¼åˆä¿¡æ¯å›ç­”
3. é€‚å½“å¼•ç”¨æ¥æº
4. å¦‚æœæœç´¢ç»“æœä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯šå®è¯´æ˜
5. å›ç­”è¦å‡†ç¡®ã€æœ‰å¸®åŠ©

æœç´¢ç»“æœ:
{search_results}

å†å²è®°å¿†:
{memory_context}"""),
                ("human", "{input}")
            ])
            
            chain = response_prompt | self.llm
            full_response = ""
            
            try:
                for chunk in chain.stream({"search_results": results_text, "memory_context": memory_context, "input": user_input}):
                    if hasattr(chunk, 'content') and chunk.content:
                        full_response += chunk.content
                        yield {"type": StreamEvent.RESPONSE_CHUNK, "content": chunk.content}
                
                self.memory_store.add_memory(user_input, full_response)
                
                yield {
                    "type": StreamEvent.RESPONSE_END,
                    "content": "",
                    "tot_score": 0.0,
                    "deep_think": False
                }
            except Exception as e:
                yield {"type": StreamEvent.ERROR, "content": f"ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}"}

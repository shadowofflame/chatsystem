"""
LangGraph Agent - æ™ºèƒ½å¯¹è¯ä»£ç†
ä½¿ç”¨ LangGraph å®ç°çŠ¶æ€æœºæ¶æ„ï¼Œæ”¯æŒå¤šç§å·¥å…·è°ƒç”¨
"""

import json
from typing import TypedDict, Annotated, Sequence, Literal
from datetime import datetime

from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser

from tools import FileHandler, WebSearcher, Calculator
from memory_store import MemoryStore


class AgentState(TypedDict):
    """AgentçŠ¶æ€å®šä¹‰"""
    messages: Annotated[Sequence[BaseMessage], "å¯¹è¯æ¶ˆæ¯åˆ—è¡¨"]
    user_input: str  # ç”¨æˆ·è¾“å…¥
    next_action: str  # ä¸‹ä¸€æ­¥åŠ¨ä½œ: chat, search, file_operation, calculate, end
    tool_calls: list  # å·¥å…·è°ƒç”¨åˆ—è¡¨
    tool_results: list  # å·¥å…·ç»“æœåˆ—è¡¨
    memory_context: str  # è®°å¿†ä¸Šä¸‹æ–‡
    final_response: str  # æœ€ç»ˆå“åº”
    needs_web_search: bool  # æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
    needs_file_operation: bool  # æ˜¯å¦éœ€è¦æ–‡ä»¶æ“ä½œ
    needs_calculation: bool  # æ˜¯å¦éœ€è¦è®¡ç®—


class LangGraphAgent:
    """
    åŸºäº LangGraph çš„æ™ºèƒ½å¯¹è¯ä»£ç†
    
    å·¥ä½œæµç¨‹:
    1. æ¥æ”¶ç”¨æˆ·è¾“å…¥
    2. åˆ†ææ„å›¾ (è·¯ç”±èŠ‚ç‚¹)
    3. æ‰§è¡Œç›¸åº”æ“ä½œ:
       - ç½‘ç»œæœç´¢
       - æ–‡ä»¶æ“ä½œ
       - æ•°å­¦è®¡ç®—
       - æ™®é€šå¯¹è¯
    4. æ•´åˆç»“æœå¹¶è¿”å›
    """
    
    def __init__(
        self,
        api_key: str = None,
        base_url: str = None,
        model: str = "deepseek-chat",
        memory_dir: str = "./memory_db",
        workspace_dir: str = "./workspace"
    ):
        """
        åˆå§‹åŒ– LangGraph Agent
        
        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
            memory_dir: è®°å¿†å­˜å‚¨ç›®å½•
            workspace_dir: å·¥ä½œç©ºé—´ç›®å½•
        """
        import os
        api_key = api_key or os.getenv("OPENAI_API_KEY")
        base_url = base_url or os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com")
        
        # åˆå§‹åŒ– LLM
        self.llm = ChatOpenAI(
            model=model,
            api_key=api_key,
            base_url=base_url,
            temperature=0.7,
            max_tokens=2000
        )
        
        # åˆå§‹åŒ–å·¥å…·
        self.file_handler = FileHandler(workspace_dir)
        self.web_searcher = WebSearcher()
        self.calculator = Calculator()
        
        # åˆå§‹åŒ–è®°å¿†
        self.memory_store = MemoryStore(persist_directory=memory_dir)
        
        # æ„å»ºçŠ¶æ€å›¾
        self.graph = self._build_graph()
        self.app = self.graph.compile()
    
    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph çŠ¶æ€å›¾"""
        
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("analyze_intent", self._analyze_intent)  # æ„å›¾åˆ†æ
        workflow.add_node("retrieve_memory", self._retrieve_memory)  # æ£€ç´¢è®°å¿†
        workflow.add_node("web_search", self._web_search)  # ç½‘ç»œæœç´¢
        workflow.add_node("file_operation", self._file_operation)  # æ–‡ä»¶æ“ä½œ
        workflow.add_node("calculate", self._calculate)  # è®¡ç®—
        workflow.add_node("generate_response", self._generate_response)  # ç”Ÿæˆå“åº”
        workflow.add_node("save_memory", self._save_memory)  # ä¿å­˜è®°å¿†
        
        # è®¾ç½®å…¥å£
        workflow.set_entry_point("analyze_intent")
        
        # æ·»åŠ æ¡ä»¶è¾¹ï¼šæ ¹æ®æ„å›¾è·¯ç”±
        workflow.add_conditional_edges(
            "analyze_intent",
            self._route_decision,
            {
                "memory": "retrieve_memory",
                "search": "web_search",
                "file": "file_operation",
                "calculate": "calculate",
                "chat": "retrieve_memory"
            }
        )
        
        # ä»è®°å¿†æ£€ç´¢åˆ°å“åº”ç”Ÿæˆ
        workflow.add_edge("retrieve_memory", "generate_response")
        
        # ä»å·¥å…·èŠ‚ç‚¹åˆ°å“åº”ç”Ÿæˆ
        workflow.add_edge("web_search", "generate_response")
        workflow.add_edge("file_operation", "generate_response")
        workflow.add_edge("calculate", "generate_response")
        
        # ä»å“åº”ç”Ÿæˆåˆ°ä¿å­˜è®°å¿†
        workflow.add_edge("generate_response", "save_memory")
        
        # ä»ä¿å­˜è®°å¿†åˆ°ç»“æŸ
        workflow.add_edge("save_memory", END)
        
        return workflow
    
    def _analyze_intent(self, state: AgentState) -> AgentState:
        """
        åˆ†æç”¨æˆ·æ„å›¾
        
        åˆ¤æ–­ç”¨æˆ·éœ€è¦:
        - ç½‘ç»œæœç´¢ (æœ€æ–°ä¿¡æ¯ã€æ–°é—»ã€å®æ—¶æ•°æ®)
        - æ–‡ä»¶æ“ä½œ (è¯»å†™æ–‡ä»¶ã€æŸ¥çœ‹ç›®å½•)
        - è®¡ç®— (æ•°å­¦è®¡ç®—ã€æ•°æ®å¤„ç†)
        - æ™®é€šå¯¹è¯
        """
        user_input = state["user_input"]
        
        # ä½¿ç”¨ LLM åˆ†ææ„å›¾
        intent_prompt = f"""åˆ†æç”¨æˆ·çš„æ„å›¾ï¼Œåˆ¤æ–­éœ€è¦æ‰§è¡Œä»€ä¹ˆæ“ä½œã€‚

ç”¨æˆ·è¾“å…¥: {user_input}

è¯·åˆ¤æ–­ç”¨æˆ·çš„æ„å›¾å¹¶è¿”å›JSONæ ¼å¼:
{{
    "intent": "search|file|calculate|chat",
    "reason": "åˆ¤æ–­ç†ç”±",
    "needs_web_search": true/false,
    "needs_file_operation": true/false,
    "needs_calculation": true/false
}}

åˆ¤æ–­æ ‡å‡†:
- search: éœ€è¦æœ€æ–°ä¿¡æ¯ã€æ–°é—»ã€å®æ—¶æ•°æ®ã€å¤©æ°”ç­‰
- file: æ¶‰åŠè¯»å†™æ–‡ä»¶ã€æŸ¥çœ‹ç›®å½•ã€ä¿å­˜å†…å®¹ç­‰
- calculate: éœ€è¦æ•°å­¦è®¡ç®—ã€æ•°æ®å¤„ç†
- chat: æ™®é€šå¯¹è¯ã€å›ç­”çŸ¥è¯†æ€§é—®é¢˜

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            response = self.llm.invoke([HumanMessage(content=intent_prompt)])
            intent_data = json.loads(response.content)
            
            state["next_action"] = intent_data.get("intent", "chat")
            state["needs_web_search"] = intent_data.get("needs_web_search", False)
            state["needs_file_operation"] = intent_data.get("needs_file_operation", False)
            state["needs_calculation"] = intent_data.get("needs_calculation", False)
            
            print(f"ğŸ” æ„å›¾åˆ†æ: {intent_data.get('intent')} - {intent_data.get('reason')}")
            
        except Exception as e:
            print(f"âš ï¸ æ„å›¾åˆ†æå¤±è´¥ï¼Œé»˜è®¤ä¸ºæ™®é€šå¯¹è¯: {e}")
            state["next_action"] = "chat"
            state["needs_web_search"] = False
            state["needs_file_operation"] = False
            state["needs_calculation"] = False
        
        return state
    
    def _route_decision(self, state: AgentState) -> Literal["memory", "search", "file", "calculate", "chat"]:
        """è·¯ç”±å†³ç­–"""
        if state.get("needs_web_search"):
            return "search"
        elif state.get("needs_file_operation"):
            return "file"
        elif state.get("needs_calculation"):
            return "calculate"
        else:
            return "memory"
    
    def _retrieve_memory(self, state: AgentState) -> AgentState:
        """æ£€ç´¢ç›¸å…³è®°å¿†"""
        user_input = state["user_input"]
        
        # æ£€ç´¢ç›¸å…³è®°å¿†
        relevant_memories = self.memory_store.search_memories(user_input, n_results=5)
        
        if relevant_memories:
            memory_context = "ã€ç›¸å…³å†å²è®°å¿†ã€‘\n"
            for i, memory in enumerate(relevant_memories, 1):
                memory_context += f"{i}. {memory['content']}\n"
        else:
            memory_context = "ï¼ˆæš‚æ— ç›¸å…³å†å²è®°å¿†ï¼‰"
        
        state["memory_context"] = memory_context
        print(f"ğŸ“š æ£€ç´¢åˆ° {len(relevant_memories)} æ¡ç›¸å…³è®°å¿†")
        
        return state
    
    def _web_search(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œç½‘ç»œæœç´¢"""
        user_input = state["user_input"]
        
        print(f"ğŸŒ æ‰§è¡Œç½‘ç»œæœç´¢: {user_input}")
        search_result = self.web_searcher.search(user_input, num_results=5)
        
        if search_result["success"]:
            results_text = "ã€ç½‘ç»œæœç´¢ç»“æœã€‘\n"
            for i, result in enumerate(search_result["results"], 1):
                results_text += f"{i}. {result['title']}\n"
                results_text += f"   {result['snippet']}\n"
                results_text += f"   æ¥æº: {result['link']}\n\n"
            
            state["tool_results"] = [{"type": "search", "content": results_text}]
            state["memory_context"] = results_text
        else:
            state["tool_results"] = [{"type": "search", "content": f"æœç´¢å¤±è´¥: {search_result.get('error')}"}]
            state["memory_context"] = "æœç´¢æœªèƒ½è¿”å›ç»“æœ"
        
        return state
    
    def _file_operation(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œæ–‡ä»¶æ“ä½œ"""
        user_input = state["user_input"]
        
        print(f"ğŸ“ æ‰§è¡Œæ–‡ä»¶æ“ä½œ")
        
        # ä½¿ç”¨LLMè§£ææ–‡ä»¶æ“ä½œæ„å›¾
        file_prompt = f"""ç”¨æˆ·æƒ³è¦æ‰§è¡Œæ–‡ä»¶æ“ä½œï¼Œè¯·è§£æå…·ä½“æ“ä½œå¹¶è¿”å›JSONæ ¼å¼:

ç”¨æˆ·è¾“å…¥: {user_input}

è¿”å›æ ¼å¼:
{{
    "operation": "read|write|list|delete",
    "filepath": "æ–‡ä»¶è·¯å¾„",
    "content": "å†™å…¥å†…å®¹ï¼ˆä»…writeæ“ä½œéœ€è¦ï¼‰"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            response = self.llm.invoke([HumanMessage(content=file_prompt)])
            file_op = json.loads(response.content)
            
            operation = file_op.get("operation")
            filepath = file_op.get("filepath", "")
            
            if operation == "read":
                result = self.file_handler.read_file(filepath)
            elif operation == "write":
                content = file_op.get("content", "")
                result = self.file_handler.write_file(filepath, content)
            elif operation == "list":
                result = self.file_handler.list_files(filepath or ".")
            elif operation == "delete":
                result = self.file_handler.delete_file(filepath)
            else:
                result = {"success": False, "error": "æœªçŸ¥çš„æ–‡ä»¶æ“ä½œ"}
            
            state["tool_results"] = [{"type": "file", "content": json.dumps(result, ensure_ascii=False, indent=2)}]
            state["memory_context"] = f"æ–‡ä»¶æ“ä½œç»“æœ: {json.dumps(result, ensure_ascii=False)}"
            
        except Exception as e:
            error_msg = f"æ–‡ä»¶æ“ä½œè§£æå¤±è´¥: {str(e)}"
            state["tool_results"] = [{"type": "file", "content": error_msg}]
            state["memory_context"] = error_msg
        
        return state
    
    def _calculate(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œè®¡ç®—"""
        user_input = state["user_input"]
        
        print(f"ğŸ§® æ‰§è¡Œè®¡ç®—")
        
        # ä½¿ç”¨LLMæå–æ•°å­¦è¡¨è¾¾å¼
        calc_prompt = f"""ä»ç”¨æˆ·è¾“å…¥ä¸­æå–æ•°å­¦è¡¨è¾¾å¼å¹¶è¿”å›JSON:

ç”¨æˆ·è¾“å…¥: {user_input}

è¿”å›æ ¼å¼:
{{
    "expression": "æ•°å­¦è¡¨è¾¾å¼ï¼ˆå¦‚: 2 + 2, 3 * 5 + 10ï¼‰"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            response = self.llm.invoke([HumanMessage(content=calc_prompt)])
            calc_op = json.loads(response.content)
            
            expression = calc_op.get("expression", "")
            result = self.calculator.calculate(expression)
            
            state["tool_results"] = [{"type": "calculate", "content": json.dumps(result, ensure_ascii=False)}]
            state["memory_context"] = f"è®¡ç®—ç»“æœ: {result}"
            
        except Exception as e:
            error_msg = f"è®¡ç®—å¤±è´¥: {str(e)}"
            state["tool_results"] = [{"type": "calculate", "content": error_msg}]
            state["memory_context"] = error_msg
        
        return state
    
    def _generate_response(self, state: AgentState) -> AgentState:
        """ç”Ÿæˆæœ€ç»ˆå“åº”"""
        user_input = state["user_input"]
        memory_context = state.get("memory_context", "")
        tool_results = state.get("tool_results", [])
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        
        if memory_context:
            context_parts.append(memory_context)
        
        if tool_results:
            for result in tool_results:
                context_parts.append(f"\nã€{result['type']}å·¥å…·ç»“æœã€‘\n{result['content']}")
        
        full_context = "\n".join(context_parts)
        
        # ç”Ÿæˆå“åº”
        response_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚:
1. å¦‚æœæœ‰æœç´¢ç»“æœï¼ŒåŸºäºæœç´¢ç»“æœå›ç­”
2. å¦‚æœæœ‰æ–‡ä»¶æ“ä½œç»“æœï¼Œè¯´æ˜æ“ä½œç»“æœ
3. å¦‚æœæœ‰è®¡ç®—ç»“æœï¼Œç»™å‡ºè®¡ç®—ç­”æ¡ˆ
4. å›ç­”è¦å‡†ç¡®ã€å‹å¥½ã€æœ‰å¸®åŠ©
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯šå®è¯´æ˜

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}"""),
            ("human", "{input}")
        ])
        
        chain = response_prompt | self.llm | StrOutputParser()
        
        try:
            response = chain.invoke({
                "context": full_context,
                "input": user_input
            })
            
            state["final_response"] = response
            print(f"âœ… ç”Ÿæˆå“åº”å®Œæˆ")
            
        except Exception as e:
            state["final_response"] = f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
        
        return state
    
    def _save_memory(self, state: AgentState) -> AgentState:
        """ä¿å­˜å¯¹è¯åˆ°è®°å¿†"""
        user_input = state["user_input"]
        final_response = state.get("final_response", "")
        
        # ä¿å­˜åˆ°é•¿æœŸè®°å¿†
        self.memory_store.add_memory(user_input, final_response)
        print(f"ğŸ’¾ ä¿å­˜è®°å¿†å®Œæˆ")
        
        return state
    
    def chat(self, user_input: str) -> str:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            
        Returns:
            AIå“åº”
        """
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = {
            "messages": [],
            "user_input": user_input,
            "next_action": "",
            "tool_calls": [],
            "tool_results": [],
            "memory_context": "",
            "final_response": "",
            "needs_web_search": False,
            "needs_file_operation": False,
            "needs_calculation": False
        }
        
        # è¿è¡ŒçŠ¶æ€å›¾
        print(f"\n{'='*50}")
        print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
        print(f"{'='*50}\n")
        
        final_state = self.app.invoke(initial_state)
        
        return final_state["final_response"]
    
    def chat_with_search(self, user_input: str) -> str:
        """
        å¼ºåˆ¶ä½¿ç”¨è”ç½‘æœç´¢å¤„ç†ç”¨æˆ·è¾“å…¥
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            
        Returns:
            AIå“åº”
        """
        print(f"\n{'='*50}")
        print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {user_input}")
        print(f"ğŸŒ å¼ºåˆ¶è”ç½‘æœç´¢æ¨¡å¼")
        print(f"{'='*50}\n")
        
        # æ‰§è¡Œç½‘ç»œæœç´¢
        print(f"ğŸ” æ‰§è¡Œç½‘ç»œæœç´¢: {user_input}")
        search_result = self.web_searcher.search(user_input, num_results=5)
        
        if search_result["success"]:
            results_text = "ã€ç½‘ç»œæœç´¢ç»“æœã€‘\n"
            for i, result in enumerate(search_result["results"], 1):
                results_text += f"{i}. {result['title']}\n"
                results_text += f"   {result['snippet']}\n"
                results_text += f"   æ¥æº: {result['link']}\n\n"
            print(f"âœ… æœç´¢æˆåŠŸï¼Œè·å– {len(search_result['results'])} æ¡ç»“æœ")
        else:
            results_text = f"æœç´¢æœªèƒ½è¿”å›ç»“æœ: {search_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            print(f"âš ï¸ æœç´¢å¤±è´¥: {search_result.get('error')}")
        
        # æ£€ç´¢ç›¸å…³è®°å¿†
        relevant_memories = self.memory_store.search_memories(user_input, n_results=3)
        memory_context = ""
        if relevant_memories:
            memory_context = "\n\nã€ç›¸å…³å†å²è®°å¿†ã€‘\n"
            for i, memory in enumerate(relevant_memories, 1):
                memory_context += f"{i}. {memory['content']}\n"
        
        # ç”Ÿæˆå“åº”
        response_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨ç½‘ç»œæœç´¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚:
1. åŸºäºæœç´¢ç»“æœå›ç­”é—®é¢˜
2. å¦‚æœ‰å¤šä¸ªæ¥æºï¼Œç»¼åˆä¿¡æ¯å›ç­”
3. é€‚å½“å¼•ç”¨æ¥æº
4. å¦‚æœæœç´¢ç»“æœä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯šå®è¯´æ˜
5. å›ç­”è¦å‡†ç¡®ã€æœ‰å¸®åŠ©

{context}"""),
            ("human", "{input}")
        ])
        
        chain = response_prompt | self.llm | StrOutputParser()
        
        try:
            response = chain.invoke({
                "context": results_text + memory_context,
                "input": user_input
            })
            print(f"âœ… ç”Ÿæˆå“åº”å®Œæˆ")
            
            # ä¿å­˜åˆ°é•¿æœŸè®°å¿†
            self.memory_store.add_memory(user_input, response)
            print(f"ğŸ’¾ ä¿å­˜è®°å¿†å®Œæˆ")
            
            return response
            
        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
            print(f"âŒ é”™è¯¯: {error_msg}")
            return error_msg
    
    def get_memory_stats(self):
        """è·å–è®°å¿†ç»Ÿè®¡"""
        return {
            "long_term_memories": self.memory_store.get_memory_count()
        }
    
    def clear_all_memory(self):
        """æ¸…é™¤æ‰€æœ‰è®°å¿†"""
        self.memory_store.clear_all_memories()
    
    def summarize(self, text: str, max_length: int = None) -> str:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œæ€»ç»“
        
        Args:
            text: éœ€è¦æ€»ç»“çš„æ–‡æœ¬
            max_length: æ€»ç»“çš„æœ€å¤§é•¿åº¦ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ€»ç»“åçš„æ–‡æœ¬
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ€»ç»“åŠ©æ‰‹ã€‚è¯·ç®€æ´ã€å‡†ç¡®åœ°æ€»ç»“ç”¨æˆ·æä¾›çš„æ–‡æœ¬ã€‚"),
            ("human", f"è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ€»ç»“ï¼š\n\n{text}" + (f"\n\nè¦æ±‚ï¼šæ€»ç»“é•¿åº¦ä¸è¶…è¿‡{max_length}å­—ã€‚" if max_length else ""))
        ])
        
        chain = prompt | self.llm | StrOutputParser()
        
        try:
            return chain.invoke({})
        except Exception as e:
            return f"æ€»ç»“å¤±è´¥: {str(e)}"
    
    def extract_information(self, text: str) -> str:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯
        
        Args:
            text: éœ€è¦æå–ä¿¡æ¯çš„æ–‡æœ¬
            
        Returns:
            æå–çš„å…³é”®ä¿¡æ¯
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·ä»ç”¨æˆ·æä¾›çš„æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬äººç‰©ã€æ—¶é—´ã€åœ°ç‚¹ã€äº‹ä»¶ç­‰é‡è¦å†…å®¹ã€‚"),
            ("human", f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯ï¼š\n\n{text}")
        ])
        
        chain = prompt | self.llm | StrOutputParser()
        
        try:
            return chain.invoke({})
        except Exception as e:
            return f"ä¿¡æ¯æå–å¤±è´¥: {str(e)}"
    
    def translate(self, text: str, target_language: str = "English") -> str:
        """
        ç¿»è¯‘æ–‡æœ¬
        
        Args:
            text: éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬
            target_language: ç›®æ ‡è¯­è¨€
            
        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·æä¾›çš„æ–‡æœ¬å‡†ç¡®ç¿»è¯‘æˆ{target_language}ã€‚åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚"),
            ("human", text)
        ])
        
        chain = prompt | self.llm | StrOutputParser()
        
        try:
            return chain.invoke({})
        except Exception as e:
            return f"ç¿»è¯‘å¤±è´¥: {str(e)}"
